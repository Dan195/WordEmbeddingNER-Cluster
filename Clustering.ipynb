{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clustering after NB 0**\n",
    "\n",
    "In the first step, we will try clustering on the prelabeled data from notebook 0. At this stage, the only processing done to the raw text is lammentizing and masking.\n",
    "\n",
    "A list of different clustering algorithms available in sklearn can be seen here\n",
    "https://scikit-learn.org/stable/modules/clustering.html\n",
    "\n",
    "x-means is also another potential solution where k is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "test = pd.read_table(\"../data/polymers.test\",encoding=\"utf-8\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(df):\n",
    "    \"\"\"processes dfs where original file \n",
    "    is of the form [label, sentence] \"\"\"\n",
    "    df = pd.DataFrame(df[0].str.split(\" \",1))\n",
    "    df[\"ground_truth\"] = df[0].str[0].apply(lambda x: 1 if x=='__label__yes' else 0)\n",
    "    df['sentence'] = df[0].str[1].apply(lambda x: x.lstrip('b'))\n",
    "    del df[0]\n",
    "    return df\n",
    "\n",
    "df = process_df(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>'Enantiopure Isotactic PCHC Synthesized by Rin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>'William Guerin\\xe2\\x80\\xa0, Abdou Khadri Dial...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>'Macromolecules, 2014, 47 (13), pp 4230\\xe2\\x8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>'DOI: 10.1021/ma5009397'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>'Publication Date (Web): June 24, 2014'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ground_truth                                           sentence\n",
       "0             0  'Enantiopure Isotactic PCHC Synthesized by Rin...\n",
       "1             0  'William Guerin\\xe2\\x80\\xa0, Abdou Khadri Dial...\n",
       "2             0  'Macromolecules, 2014, 47 (13), pp 4230\\xe2\\x8...\n",
       "3             0                           'DOI: 10.1021/ma5009397'\n",
       "4             0            'Publication Date (Web): June 24, 2014'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    28241\n",
       "1     4349\n",
       "Name: ground_truth, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ground_truth.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the above, the majority cases in the dataset do not contain a polymer. First, let's try stemming on the\n",
    "entire sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following will explore simple count vectorization on kmeans to see if any trends can be found\n",
    "for clusters without processing done beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def porter_stem(string):\n",
    "    \n",
    "    list_string = string.split(\" \")\n",
    "    \n",
    "    for i in range(len(list_string)):\n",
    "        word = list_string[i]\n",
    "        if word not in stopwords.words('english'):\n",
    "            list_string[i] = ps.stem(list_string[i])\n",
    "        \n",
    "    return \" \".join(list_string)\n",
    "\n",
    "df[\"porter_sentence\"] = df.sentence.map(porter_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>sentence</th>\n",
       "      <th>porter_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>'Enantiopure Isotactic PCHC Synthesized by Rin...</td>\n",
       "      <td>'enantiopur isotact pchc synthes by ring-open ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>'William Guerin\\xe2\\x80\\xa0, Abdou Khadri Dial...</td>\n",
       "      <td>'william guerin\\xe2\\x80\\xa0, abdou khadri dial...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>'Macromolecules, 2014, 47 (13), pp 4230\\xe2\\x8...</td>\n",
       "      <td>'macromolecules, 2014, 47 (13), pp 4230\\xe2\\x8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>'DOI: 10.1021/ma5009397'</td>\n",
       "      <td>'doi: 10.1021/ma5009397'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>'Publication Date (Web): June 24, 2014'</td>\n",
       "      <td>'public date (web): june 24, 2014'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ground_truth                                           sentence  \\\n",
       "0             0  'Enantiopure Isotactic PCHC Synthesized by Rin...   \n",
       "1             0  'William Guerin\\xe2\\x80\\xa0, Abdou Khadri Dial...   \n",
       "2             0  'Macromolecules, 2014, 47 (13), pp 4230\\xe2\\x8...   \n",
       "3             0                           'DOI: 10.1021/ma5009397'   \n",
       "4             0            'Publication Date (Web): June 24, 2014'   \n",
       "\n",
       "                                     porter_sentence  \n",
       "0  'enantiopur isotact pchc synthes by ring-open ...  \n",
       "1  'william guerin\\xe2\\x80\\xa0, abdou khadri dial...  \n",
       "2  'macromolecules, 2014, 47 (13), pp 4230\\xe2\\x8...  \n",
       "3                           'doi: 10.1021/ma5009397'  \n",
       "4                 'public date (web): june 24, 2014'  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-e25cfdb505fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m kmeans_runs = KMeansRunner().run(n_grams,n_clusters,df.porter_sentence,df.ground_truth,CountVectorizer,\n\u001b[0;32m      8\u001b[0m                                  \u001b[0mmin_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m                                  min_threshold=0.5)\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\MS_Data_Science\\NLPResearch\\WordEmbeddingNER-master\\WordEmbeddingNER-master\\clustering\\ClusterRunner.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, n_grams, n_clusters, sentences, ground_truth, vectorizer_class, min_size, min_threshold)\u001b[0m\n\u001b[0;32m     73\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 \u001b[0mkmeans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m                 \u001b[0mall_cluster_h_scores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_gram\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhomogeneity_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mground_truth\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m                 \u001b[0mall_cluster_s_scores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_gram\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msilhouette_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    970\u001b[0m                 \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy_x\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    971\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 972\u001b[1;33m                 return_n_iter=True)\n\u001b[0m\u001b[0;32m    973\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    974\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36mk_means\u001b[1;34m(X, n_clusters, sample_weight, init, precompute_distances, n_init, max_iter, verbose, tol, random_state, copy_x, n_jobs, algorithm, return_n_iter)\u001b[0m\n\u001b[0;32m    379\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecompute_distances\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprecompute_distances\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m                 \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_squared_norms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_squared_norms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m                 random_state=random_state)\n\u001b[0m\u001b[0;32m    382\u001b[0m             \u001b[1;31m# determine if these results are the best so far\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbest_inertia\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0minertia\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mbest_inertia\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36m_kmeans_single_lloyd\u001b[1;34m(X, sample_weight, n_clusters, max_iter, init, verbose, x_squared_norms, random_state, tol, precompute_distances)\u001b[0m\n\u001b[0;32m    550\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m             centers = _k_means._centers_sparse(X, sample_weight, labels,\n\u001b[1;32m--> 552\u001b[1;33m                                                n_clusters, distances)\n\u001b[0m\u001b[0;32m    553\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m             centers = _k_means._centers_dense(X, sample_weight, labels,\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ClusterRunner import KMeansRunner\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "n_grams = [(1,2),(2,2),(1,3),(3,3)]\n",
    "n_clusters = range(2,5)\n",
    "\n",
    "kmeans_runs = KMeansRunner().run(n_grams,n_clusters,df.porter_sentence,df.ground_truth,CountVectorizer,\n",
    "                                 min_size = 300,\n",
    "                                 min_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'homogeneity_all': {(1, 1): {2: 0.00574091569911332,\n",
       "   3: 0.008260383068843349,\n",
       "   4: 0.01026701108749708},\n",
       "  (1, 2): {2: 0.006239336568233691,\n",
       "   3: 0.007263586730323506,\n",
       "   4: 0.010546138160955494},\n",
       "  (2, 2): {2: 0.0017193928027511185,\n",
       "   3: 0.0032423801075513138,\n",
       "   4: 0.005917589377338861},\n",
       "  (1, 3): {2: 0.006219906665485889,\n",
       "   3: 0.007181215932669023,\n",
       "   4: 0.010554662668325453},\n",
       "  (3, 3): {2: 0.0002733894195257013,\n",
       "   3: 5.952706844845892e-05,\n",
       "   4: 0.00115413652394407}},\n",
       " 'homogeneity_single': {(1,\n",
       "   1): {2: [(1, 0.8821279245618717, 'no'),\n",
       "    (0, 0.831882050267168, 'no')], 3: [(1, 0.8853747630276969, 'no'), (0,\n",
       "     0.7776726584673604,\n",
       "     'no'), (2, 0.8348985156013329, 'no')], 4: [(3, 0.895406680018089, 'no'),\n",
       "    (1, 0.77734375, 'no'),\n",
       "    (0, 0.8513195290296387, 'no'),\n",
       "    (2, 0.8219214437367304, 'no')]},\n",
       "  (1, 2): {2: [(1, 0.8823812439856531, 'no'), (0, 0.8293234628829941, 'no')],\n",
       "   3: [(0, 0.8845057471264368, 'no'),\n",
       "    (1, 0.8048359240069085, 'no'),\n",
       "    (2, 0.8353846153846154, 'no')],\n",
       "   4: [(0, 0.8955592105263158, 'no'),\n",
       "    (2, 0.7747844827586207, 'no'),\n",
       "    (3, 0.8495924919733268, 'no'),\n",
       "    (1, 0.8212260329462598, 'no')]},\n",
       "  (2, 2): {2: [(0, 0.869541361391731, 'no'), (1, 0.8087227414330218, 'no')],\n",
       "   3: [(1, 0.8759172231841043, 'no'),\n",
       "    (2, 0.8081063964534515, 'no'),\n",
       "    (0, 0.8465185185185186, 'no')],\n",
       "   4: [(0, 0.8743307301435733, 'no'),\n",
       "    (3, 0.7891373801916933, 'no'),\n",
       "    (2, 0.8037456106125634, 'no')]},\n",
       "  (1, 3): {2: [(1, 0.8823298086166215, 'no'), (0, 0.8293135435992579, 'no')],\n",
       "   3: [(0, 0.8844526218951242, 'no'),\n",
       "    (1, 0.8061342592592593, 'no'),\n",
       "    (2, 0.8352889571224915, 'no')],\n",
       "   4: [(1, 0.8954912856782016, 'no'),\n",
       "    (2, 0.7742980561555075, 'no'),\n",
       "    (0, 0.8496011841131486, 'no'),\n",
       "    (3, 0.8209118209118209, 'no')]},\n",
       "  (3, 3): {2: [(1, 0.8671115939779606, 'no'), (0, 0.8176943699731903, 'no')],\n",
       "   3: [(2, 0.8665599803040561, 'no')],\n",
       "   4: [(3, 0.8662112680390166, 'no')]}},\n",
       " 'silhouette': {(1, 1): {2: 0.15377133114525543,\n",
       "   3: 0.14530411002233629,\n",
       "   4: 0.06152310605549069},\n",
       "  (1, 2): {2: 0.1348529816194494,\n",
       "   3: 0.12272932820082882,\n",
       "   4: 0.0446397705587366},\n",
       "  (2, 2): {2: 0.24449483889421145,\n",
       "   3: 0.07502657604450053,\n",
       "   4: 0.13254890025095417},\n",
       "  (1, 3): {2: 0.12608255855485645,\n",
       "   3: 0.11276371055884335,\n",
       "   4: 0.0364692785753034},\n",
       "  (3, 3): {2: 0.21640703373863932,\n",
       "   3: 0.09754460396744714,\n",
       "   4: -0.10065235585364761}}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " kmeans_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output, it can be seen that no amount of clusters found a subset of data that heavily consisted of polymers or non-polymers as each cluster was predominantly non-polymers, but not to the extent that it was a significant higher concentration than the overall population. Next, the above will be applied with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "2\n",
      "3\n",
      "4\n",
      "(2, 2)\n",
      "2\n",
      "3\n",
      "4\n",
      "(1, 3)\n",
      "2\n",
      "3\n",
      "4\n",
      "(3, 3)\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "from ClusterRunner import KMeansRunner\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "n_grams = [(1,2),(2,2),(1,3),(3,3)]\n",
    "n_clusters = range(2,5)\n",
    "\n",
    "kmeans_run_tf = KMeansRunner().run(n_grams,n_clusters,df.porter_sentence,df.ground_truth,TfidfVectorizer,\n",
    "                                 min_size = 1000,\n",
    "                                 min_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'homogeneity_all': {(1, 2): {2: 0.015026917059195549,\n",
       "   3: 0.00988356356676446,\n",
       "   4: 0.013440405745402954},\n",
       "  (2, 2): {2: 0.00020589232150894726,\n",
       "   3: 0.0013393800798682093,\n",
       "   4: 0.006616275659855298},\n",
       "  (1, 3): {2: 0.012371289077022476,\n",
       "   3: 0.012800834793106097,\n",
       "   4: 0.013391848347362416},\n",
       "  (3, 3): {2: 0.006528119466355282,\n",
       "   3: 0.0020582855704386264,\n",
       "   4: 0.0034908972569753414}},\n",
       " 'homogeneity_single': {(1,\n",
       "   2): {2: [(1, 0.8609476915206549, 'no', 26927),\n",
       "    (0, 1.0, 'no', 1314)], 3: [(1, 0.8649063219020489, 'no', 23682), (2,\n",
       "     0.8508832300986465,\n",
       "     'no',\n",
       "     3709)], 4: [(2, 0.8648109167453001, 'no', 23829),\n",
       "    (1, 0.840531561461794, 'no', 3289)]},\n",
       "  (2,\n",
       "   2): {2: [(1, 0.8654200078874721, 'no', 26333),\n",
       "    (0, 0.8825161887141536, 'no', 1908)], 3: [(2,\n",
       "     0.869203282679546,\n",
       "     'no',\n",
       "     22136),\n",
       "    (0, 0.8552949538024165, 'no', 6017)], 4: [(2,\n",
       "     0.8681864929722317,\n",
       "     'no',\n",
       "     20260),\n",
       "    (0, 0.8629611883085769, 'no', 1801),\n",
       "    (3, 0.8510188679245283, 'no', 5638)]},\n",
       "  (1, 3): {2: [(1, 0.8619540375825292, 'no', 27155), (0, 1.0, 'no', 1086)],\n",
       "   3: [(2, 0.8617917183080688, 'no', 27118), (0, 1.0, 'no', 1024)],\n",
       "   4: [(2, 0.8634954569748797, 'no', 24234),\n",
       "    (1, 0.8465185185185186, 'no', 2857),\n",
       "    (0, 1.0, 'no', 1059)]},\n",
       "  (3, 3): {2: [(1, 0.8641446957390978, 'no', 27663)],\n",
       "   3: [(1, 0.8668074111419208, 'no', 27509)],\n",
       "   4: [(0, 0.8652632893997126, 'no', 27704)]}},\n",
       " 'models': {(1,\n",
       "   2): {2: KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "          n_clusters=2, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "          random_state=None, tol=0.0001, verbose=0), 3: KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "          n_clusters=3, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "          random_state=None, tol=0.0001, verbose=0), 4: KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "          n_clusters=4, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "          random_state=None, tol=0.0001, verbose=0)},\n",
       "  (2,\n",
       "   2): {2: KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "          n_clusters=2, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "          random_state=None, tol=0.0001, verbose=0), 3: KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "          n_clusters=3, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "          random_state=None, tol=0.0001, verbose=0), 4: KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "          n_clusters=4, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "          random_state=None, tol=0.0001, verbose=0)},\n",
       "  (1,\n",
       "   3): {2: KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "          n_clusters=2, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "          random_state=None, tol=0.0001, verbose=0), 3: KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "          n_clusters=3, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "          random_state=None, tol=0.0001, verbose=0), 4: KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "          n_clusters=4, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "          random_state=None, tol=0.0001, verbose=0)},\n",
       "  (3,\n",
       "   3): {2: KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "          n_clusters=2, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "          random_state=None, tol=0.0001, verbose=0), 3: KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "          n_clusters=3, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "          random_state=None, tol=0.0001, verbose=0), 4: KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "          n_clusters=4, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "          random_state=None, tol=0.0001, verbose=0)}},\n",
       " 'silhouette': {(1, 2): {2: 0.010023708471842084,\n",
       "   3: 0.008043311073743393,\n",
       "   4: 0.008774819548070245},\n",
       "  (2, 2): {2: 0.0130267538227261,\n",
       "   3: 0.012901581759586006,\n",
       "   4: 0.01197696432887535},\n",
       "  (1, 3): {2: 0.00834135960897574,\n",
       "   3: 0.01094600828722102,\n",
       "   4: 0.00876936085313837},\n",
       "  (3, 3): {2: 0.01954682270808433,\n",
       "   3: 0.020005442530341346,\n",
       "   4: 0.019563810993433246}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_run_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the porter stemmed data with TF-IDF, it can be seen that unigram-bigram 3 clusters, bigram 4 clusters, bigram 4 clusters, and unigram-trigram 2 clusters all have 100% \"No\". We can use these to potentially filter out candidates either before or after the prediction happens.\n",
    "\n",
    "The current problem that's occuring is that although 100% no clusters can be identified, it does very from each run as to where they exist. Typically they exist from bigram + , but not consistently. When they aren't deterministic for the 100% note label, then they go back to the 8x% no percentage, which makes them unusable for our purposes.\n",
    "\n",
    "One option would be to set the seed to guarantee we get the same cluster, but let's explore other options instead.\n",
    "\n",
    "Due to the above behaivor, it seems deterministic clusters might be more desirable for aiding in reducing false positives. \n",
    "Lets explore the following as listed below https://stats.stackexchange.com/questions/205833/deterministic-clustering-approaches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(2, 2)\n",
      "(1, 3)\n",
      "(3, 3)\n"
     ]
    }
   ],
   "source": [
    "from ClusterRunner import DBScanRunner\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "n_grams = [(1,2),(2,2),(1,3),(3,3)]\n",
    "#n_clusters = range(2,5)\n",
    "\n",
    "dbscan_tf = DBScanRunner().run(n_grams,df.porter_sentence,df.ground_truth,TfidfVectorizer,\n",
    "                                 dbscan_kwargs = {\"n_jobs\":-1}, # -1 for multiprocessing\n",
    "                                 min_size = 10, #setting a low min size here to start to see if any 'yes' cluster trends may show up\n",
    "                                 min_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'homogeneity_all': {(1, 2): 0.022641047925681737,\n",
       "  (2, 2): 0.02078263796314288,\n",
       "  (1, 3): 0.022557623726731556,\n",
       "  (3, 3): 0.02201095951119997},\n",
       " 'homogeneity_single': {(1, 2): [(-1, 0.8584601348929654, 'no', 26347),\n",
       "   (0, 1.0, 'no', 12),\n",
       "   (1, 1.0, 'no', 360),\n",
       "   (2, 1.0, 'no', 129),\n",
       "   (3, 1.0, 'no', 131),\n",
       "   (4, 1.0, 'no', 111),\n",
       "   (5, 1.0, 'no', 38),\n",
       "   (6, 1.0, 'no', 89),\n",
       "   (7, 1.0, 'no', 88),\n",
       "   (8, 1.0, 'no', 64),\n",
       "   (9, 1.0, 'no', 139),\n",
       "   (10, 1.0, 'no', 123),\n",
       "   (11, 1.0, 'no', 12),\n",
       "   (12, 1.0, 'no', 74),\n",
       "   (13, 1.0, 'no', 13),\n",
       "   (15, 1.0, 'no', 12),\n",
       "   (16, 1.0, 'no', 11),\n",
       "   (17, 1.0, 'no', 96),\n",
       "   (18, 1.0, 'no', 64),\n",
       "   (20, 1.0, 'no', 24),\n",
       "   (21, 1.0, 'no', 21),\n",
       "   (24, 1.0, 'no', 15),\n",
       "   (25, 1.0, 'no', 18),\n",
       "   (27, 1.0, 'no', 41),\n",
       "   (32, 1.0, 'no', 13),\n",
       "   (33, 1.0, 'no', 15),\n",
       "   (34, 1.0, 'no', 14),\n",
       "   (38, 1.0, 'no', 15)],\n",
       "  (2, 2): [(-1, 0.8580396259726673, 'no', 26244),\n",
       "   (0, 1.0, 'no', 12),\n",
       "   (1, 0.994535519125683, 'no', 1274),\n",
       "   (2, 1.0, 'no', 111),\n",
       "   (3, 1.0, 'no', 89),\n",
       "   (4, 1.0, 'no', 86),\n",
       "   (5, 1.0, 'no', 64),\n",
       "   (6, 1.0, 'no', 12),\n",
       "   (7, 1.0, 'no', 13),\n",
       "   (9, 1.0, 'no', 12),\n",
       "   (10, 1.0, 'no', 11),\n",
       "   (11, 1.0, 'no', 96),\n",
       "   (13, 1.0, 'no', 24),\n",
       "   (14, 1.0, 'no', 21),\n",
       "   (16, 1.0, 'no', 15),\n",
       "   (17, 1.0, 'no', 18),\n",
       "   (21, 1.0, 'no', 13),\n",
       "   (22, 1.0, 'no', 14),\n",
       "   (25, 1.0, 'no', 15)],\n",
       "  (1, 3): [(-1, 0.8584924099289856, 'no', 26354),\n",
       "   (0, 1.0, 'no', 12),\n",
       "   (1, 1.0, 'no', 360),\n",
       "   (2, 1.0, 'no', 129),\n",
       "   (3, 1.0, 'no', 131),\n",
       "   (4, 1.0, 'no', 111),\n",
       "   (5, 1.0, 'no', 38),\n",
       "   (6, 1.0, 'no', 89),\n",
       "   (7, 1.0, 'no', 86),\n",
       "   (8, 1.0, 'no', 64),\n",
       "   (9, 1.0, 'no', 139),\n",
       "   (10, 1.0, 'no', 123),\n",
       "   (11, 1.0, 'no', 12),\n",
       "   (12, 1.0, 'no', 74),\n",
       "   (13, 1.0, 'no', 13),\n",
       "   (15, 1.0, 'no', 12),\n",
       "   (16, 1.0, 'no', 11),\n",
       "   (17, 1.0, 'no', 96),\n",
       "   (18, 1.0, 'no', 64),\n",
       "   (20, 1.0, 'no', 24),\n",
       "   (21, 1.0, 'no', 21),\n",
       "   (24, 1.0, 'no', 15),\n",
       "   (25, 1.0, 'no', 18),\n",
       "   (27, 1.0, 'no', 41),\n",
       "   (32, 1.0, 'no', 13),\n",
       "   (33, 1.0, 'no', 15),\n",
       "   (34, 1.0, 'no', 14),\n",
       "   (38, 1.0, 'no', 15)],\n",
       "  (3, 3): [(-1, 0.8571334453338604, 'no', 26020),\n",
       "   (0, 1.0, 'no', 12),\n",
       "   (1, 0.9932165065008479, 'no', 1757),\n",
       "   (2, 1.0, 'no', 111),\n",
       "   (3, 1.0, 'no', 85),\n",
       "   (4, 1.0, 'no', 64),\n",
       "   (5, 1.0, 'no', 12),\n",
       "   (6, 1.0, 'no', 13),\n",
       "   (8, 1.0, 'no', 12),\n",
       "   (9, 1.0, 'no', 11),\n",
       "   (10, 1.0, 'no', 24),\n",
       "   (11, 1.0, 'no', 15),\n",
       "   (12, 1.0, 'no', 18),\n",
       "   (15, 1.0, 'no', 13),\n",
       "   (16, 1.0, 'no', 14),\n",
       "   (18, 1.0, 'no', 15)]},\n",
       " 'modelss': {(1,\n",
       "   2): DBSCAN(algorithm='auto', eps=0.5, leaf_size=30, metric='euclidean',\n",
       "         metric_params=None, min_samples=5, n_jobs=-1, p=None),\n",
       "  (2, 2): DBSCAN(algorithm='auto', eps=0.5, leaf_size=30, metric='euclidean',\n",
       "         metric_params=None, min_samples=5, n_jobs=-1, p=None),\n",
       "  (1, 3): DBSCAN(algorithm='auto', eps=0.5, leaf_size=30, metric='euclidean',\n",
       "         metric_params=None, min_samples=5, n_jobs=-1, p=None),\n",
       "  (3, 3): DBSCAN(algorithm='auto', eps=0.5, leaf_size=30, metric='euclidean',\n",
       "         metric_params=None, min_samples=5, n_jobs=-1, p=None)},\n",
       " 'silhouette': {(1, 2): -0.21491443617310527,\n",
       "  (2, 2): -0.21320119568038712,\n",
       "  (1, 3): -0.21650020914697005,\n",
       "  (3, 3): -0.20444076872765932}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbscan_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No majority \"yes\" clusters have been found with default params. One thing of interest is the larger 'no' cluster in\n",
    "the trigram (3,3) output. Further exploration of model tuning should be performed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
